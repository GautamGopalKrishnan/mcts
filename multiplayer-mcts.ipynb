{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv:\n",
    "    \"\"\"An environment for two-player tic-tac-toe.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.players = 2\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Initialize a new game.\"\"\"\n",
    "        self.board = np.zeros((3, 3), dtype=np.int)\n",
    "        self.turn = 0\n",
    "        self.done = False\n",
    "        self.actions = [(a, b) for a in range(3) for b in range(3)]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Perform action and return new state, rewards, done, and turn.\"\"\"\n",
    "        assert self.board[action] == 0\n",
    "        self.board[action] = (-1) ** self.turn\n",
    "        self.turn = (self.turn + 1) % 2\n",
    "        winner = self.winner(action)\n",
    "        if winner is not None:\n",
    "            rewards = np.array([winner,(-1)*winner])\n",
    "        else:\n",
    "            rewards = np.array([0, 0])\n",
    "        self.done = winner is not None or np.all(self.board != 0)\n",
    "        if self.done:\n",
    "            self.actions = []\n",
    "        else:\n",
    "            self.actions = [(a, b) for a in range(3) for b in range(3) if self.board[a, b] == 0]\n",
    "        return self.board.copy(), rewards, self.done, self.turn\n",
    "\n",
    "    def copy(self):\n",
    "        copy = TicTacToeEnv()\n",
    "        copy.board = self.board.copy()\n",
    "        copy.turn = self.turn\n",
    "        copy.done = self.done\n",
    "        copy.actions = self.actions.copy()\n",
    "        return copy\n",
    "\n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "    def winner(self,action):\n",
    "        (r,c)=(self.rsum(action[0]),self.csum(action[1]))\n",
    "        if r==3 or r==-3:\n",
    "            return np.sign(r)\n",
    "        if c==3 or c==-3:\n",
    "            return np.sign(c)\n",
    "        if (action[0]==action[1]):\n",
    "            d=self.dsum()\n",
    "            if d==3 or d==-3:\n",
    "                return np.sign(d)\n",
    "        if (sum(action)==2):\n",
    "            a=self.adsum()\n",
    "            if a==3 or a==-3:\n",
    "                return np.sign(a)\n",
    "        return None\n",
    "    \n",
    "    def rsum(self,r=0):\n",
    "        return np.sum(self.board[r, :])\n",
    "    \n",
    "    def csum(self,c=0):\n",
    "        return np.sum(self.board[:, c])\n",
    "    \n",
    "    def dsum(self):\n",
    "        return np.sum(self.board[np.arange(3), np.arange(3)])\n",
    "        \n",
    "    def adsum(self):\n",
    "        return np.sum(self.board[np.arange(3), np.arange(2, -1, -1)])\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return np.array_equal(self.board, other.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(epsilon=0.05):\n",
    "    \"\"\"Return an epsilon-greedy tree policy.\"\"\"\n",
    "    def policy(node):\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(node.children)\n",
    "        else:\n",
    "            return max(node.children, key=lambda n: n.value[node.env.turn])\n",
    "    return policy\n",
    "\n",
    "\n",
    "def ucb(c=np.sqrt(2)):\n",
    "    \"\"\"Return an upper confidence bound tree policy.\"\"\"\n",
    "    def policy(node):\n",
    "        def v(n):\n",
    "            if n.visits == 0:\n",
    "                return np.inf\n",
    "            else:\n",
    "                return n.value[node.env.turn] + c * np.sqrt(np.log(node.visits)/n.visits)\n",
    "        return max(node.children, key=v)\n",
    "    return policy\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"A tree node for Monte Carlo tree search.\"\"\"\n",
    "\n",
    "    def __init__(self, parent, action, reward, env):\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.env = env\n",
    "        self.visits = 0\n",
    "        self.value = np.zeros(env.players)\n",
    "\n",
    "\n",
    "class MCTSAgent:\n",
    "    \"\"\"A Monte Carlo tree search agent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree_policy : function\n",
    "        A function which maps node to child node.\n",
    "    timeout : float, optional\n",
    "        The amount of time in seconds to perform rollouts before choosing an action.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tree_policy=ucb(), timeout=1.0):\n",
    "        self.tree_policy = tree_policy\n",
    "        self.timeout = timeout\n",
    "        self.root = None\n",
    "\n",
    "    def act(self, env):\n",
    "        \"\"\"Return a chosen action for the env.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : environment\n",
    "            The current environment.\n",
    "\n",
    "        \"\"\"\n",
    "        self.root = self.find_root(env)\n",
    "        limit = time.time() + self.timeout\n",
    "        while time.time() < limit:\n",
    "            leaf = self.expand(self.root)\n",
    "            value = self.simulate(leaf)\n",
    "            self.backup(leaf, value)\n",
    "        return max(self.root.children, key=lambda node: node.visits).action\n",
    "    \n",
    "    def expand(self, node):\n",
    "        \"\"\"Return an unvisited or terminal leaf node following the tree policy.\n",
    "\n",
    "        Before returning, this function performs all possible actions from the\n",
    "        leaf node and adds new nodes for them to the tree as children of the\n",
    "        leaf node.\n",
    "        \"\"\"\n",
    "        while node.visits != 0 and len(node.children) > 0:\n",
    "            node = self.tree_policy(node)\n",
    "        if not node.env.done:\n",
    "            for action in node.env.actions:\n",
    "                env = node.env.copy()\n",
    "                _, reward, _, _ = env.step(action)\n",
    "                node.children.append(TreeNode(node, action, reward, env))\n",
    "        return node\n",
    "\n",
    "    def simulate(self, node):\n",
    "        \"\"\"Return one total reward from node following uniform random policy.\"\"\"\n",
    "        env = node.env.copy()\n",
    "        total_rewards = np.zeros(env.players)\n",
    "        while not env.done:\n",
    "            action = random.choice(env.actions)\n",
    "            _, rewards, _, _ = env.step(action)\n",
    "            total_rewards += rewards\n",
    "        return total_rewards\n",
    "\n",
    "    def backup(self, node, value):\n",
    "        \"\"\"Backup the return from a rollout from node.\"\"\"\n",
    "        while node != None:\n",
    "            value += node.reward\n",
    "            node.visits += 1\n",
    "            node.value = (node.visits - 1)/node.visits * node.value + value/node.visits\n",
    "            node = node.parent\n",
    "\n",
    "    def find_root(self, env):\n",
    "        \"\"\"Return node corresponding to env in current tree using BFS.\"\"\"\n",
    "        if self.root is not None:\n",
    "            q = deque(self.root.children)\n",
    "            while q:\n",
    "                node = q.popleft()\n",
    "                if node.env == env:\n",
    "                    return node\n",
    "                q.extend(node.children)\n",
    "        return TreeNode(None, None, np.zeros(env.players), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"An agent that picks an action uniformly at random.\"\"\"\n",
    "\n",
    "    def act(self, env):\n",
    "        return random.choice(env.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanAgent:\n",
    "    \"\"\"An agent controlled by a human player's input.\"\"\"\n",
    "\n",
    "    def act(self, env):\n",
    "        indices = input('Input action: ').replace('(', '').replace(')', '').split(',')\n",
    "        return tuple(int(x) for x in indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agents, env, render=False):\n",
    "    \"\"\"Run agents on env and return total rewards.\"\"\"\n",
    "    env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = np.zeros(len(agents))\n",
    "    while not env.done:\n",
    "        action = agents[env.turn].act(env)\n",
    "        _, rewards, _, _ = env.step(action)\n",
    "        total_reward += rewards\n",
    "        if render:\n",
    "            env.render()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEnv()\n",
    "agents = [RandomAgent(), MCTSAgent(timeout=.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.5 s, sys: 55.4 ms, total: 33.6 s\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "%time returns = np.array([run_episode(agents, env) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 87])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(returns == 1).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEnv()\n",
    "agents = [HumanAgent(), MCTSAgent(timeout=1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Input action: (1,1)\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "[[-1  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "Input action: (2,2)\n",
      "[[-1  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  1]]\n",
      "[[-1  0 -1]\n",
      " [ 0  1  0]\n",
      " [ 0  0  1]]\n",
      "Input action: (0,1)\n",
      "[[-1  1 -1]\n",
      " [ 0  1  0]\n",
      " [ 0  0  1]]\n",
      "[[-1  1 -1]\n",
      " [ 0  1  0]\n",
      " [ 0 -1  1]]\n",
      "Input action: (1,0)\n",
      "[[-1  1 -1]\n",
      " [ 1  1  0]\n",
      " [ 0 -1  1]]\n",
      "[[-1  1 -1]\n",
      " [ 1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "Input action: (2,0)\n",
      "[[-1  1 -1]\n",
      " [ 1  1 -1]\n",
      " [ 1 -1  1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(agents, env, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
